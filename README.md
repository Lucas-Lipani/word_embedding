# Tremplin Recherche Project – Semantic and Structural Analysis of Documents

## General Objective

This project explores how different representations, **structural (SBM)** and **semantic (Word2Vec)**, can be used to analyze large document corpora in the context of social science research. It builds heterogeneous graphs connecting documents with terms, and/or context windows, and compares partitions obtained via **Bayesian Stochastic Block Models (SBM)** with those obtained via **Word2Vec + KMeans**. The goal is to understand how **topological communities** and **semantic clusters** of terms align or diverge.

The project is part of the **Cortext** platform (LISIS, CNRS, INRAE, UGE) research initiative, which provides advanced computational tools to researchers in the social sciences.

---

## Key Files & Responsibilities

| File | Role |
|------|------|
| `window_experiments.py` | Generates partitions for all windows and runs |
| `config_manager.py` | Manages `CONFIG` directory creation and metadata |
| `results_io.py` | Saves partition data to the new structure |
| `compute_partition_metrics.py` | Computes and aggregates metrics across all runs |
| `compute_detailed_run_comparisons.py` | Run-by-run detailed comparison and heatmaps |
| `plot_average_heatmap.py` | Plots aggregated metrics as heatmaps |
| `sample_partitions.py` | Inspects and exports partition terms |
| `graph_build.py` | Constructs Document–Window–Term graphs |
| `graph_sbm.py` | Applies SBM inference |
| `w2vec_kmeans.py` | Trains Word2Vec and clusters with KMeans |

---

## Current Pipeline

The code now implements a **modular, multi-run experimental pipeline** with improved seed control, averaging, and visualization:

### `window_experiments.py` — Partition generation

* Selects a fixed number of documents from the corpus (`--samples`) using a user-defined `--seed`.
* For each run:
  * Builds **Document–Window–Term** graphs for multiple window sizes, (another graphic model can be chosen).
  * Applies SBM to the **Context–Window–Term** graph to extract term blocks.
  * Uses the number of SBM blocks to guide **KMeans** clustering on Word2Vec embeddings.
  * Saves **partition files** (`partitions_runXXX.parquet`) under `outputs/conf/NNNN/RUN/NNNN/`.

### `compute_partition_metrics.py` — Metric computation

* Reconstructs all partition runs from saved files (grouped by seed).
* Compares:
  * **SBM × W2V**
  * **SBM × SBM**
  * **W2V × W2V**
* Computes **VI**, **NMI**, **ARI** and other metrics  for each pair of windows and saves:
  * One `metrics_runXXX.parquet` per run
  * One aggregated `running_mean.parquet` per seed with full cross-window results

### `plot_average_heatmap.py` — Visualization

* Parses `running_mean.parquet`
* Generates heatmaps for each comparison type and metric:
  * `mean_nmi_sbm_w2v.png`
  * `mean_ari_sbm_sbm.png`
  * `mean_vi_w2v_w2v.png`
* Each heatmap shows **window × window** comparisons for the chosen metric

---

## Theoretical Foundation

### **Stochastic Block Model (SBM)**

SBM is a **generative probabilistic model** that discovers community structure in networks by inferring a block assignment for each vertex. It assumes that:

- The graph is generated by assigning vertices to **latent blocks** (communities)
- Edge probability depends only on the blocks of its endpoints
- The model maximizes likelihood while penalizing complexity (MDL principle)

**What it captures:** Topological communities based on **structural similarity** — vertices with similar connection patterns are grouped together, regardless of node attributes.

**Interpretation:** Two terms in the same SBM block are **structurally similar** in how they relate to documents and context windows.

---

### **Word2Vec (Skip-gram)**
Word2Vec is a **neural embedding model** that learns distributed representations of words by:

- Training a shallow 2-layer neural network on a prediction task
- Skip-gram variant: predicting context words from a target word
- Optimizing to maximize similarity between related words in embedding spacem

**What it captures:** **Semantic similarity** based on distributional hypothesis — words appearing in similar contexts have similar meanings.

**Interpretation:** Two terms close in Word2Vec embedding space are **semantically similar** in their linguistic context.

---

### **KMeans Clustering**
KMeans partitions Word2Vec embeddings into **K convex clusters** by:m

- Initializing K cluster centers randomly
- Assigning points to nearest center (Euclidean distance)
- Iteratively updating centers until convergence

**What it captures:** **Semantic clustering** of terms based on embedding similarity, independent of graph topology.

**Interpretation:** Terms in the same KMeans cluster share **semantic neighborhoods** in the embedding space.

The project aims to **compare and contrast** these methods to uncover divergences and complementarities between **structure** and **semantics** in language.

---

## Information-Theoretic Metrics

| Metric | Range | Interpretation | Use Case |
|------|------|---------------|---------|
| VI (Variation of Information) | [0, ∞) | Distance between partitions (information-theoretic divergence). Lower = more similar. | Primary metric for partition distance; symmetric and metric-compliant. |
| NVI (Normalized VI) | [0, 1] | VI normalized by entropy of partitions. Lower = more similar. | Comparable across different numbers of clusters; corrects for partition size. |
| MI (Mutual Information) | [0, min(H(X), H(Y))] | Shared information between two partitions. Higher = more similar. | Measures redundancy; captures common structure. |
| AMI (Adjusted Mutual Information) | [-1, 1] | MI adjusted for chance agreement (like Cohen's kappa). Higher = more similar. | Accounts for random clustering; 0 = random agreement, 1 = perfect agreement. |
| NMI (Normalized Mutual Information) | [0, 1] | MI normalized by average entropy. Higher = more similar. | Gold-standard metric; widely used in clustering evaluation; intuitive [0,1] scale. |
| ANMI (Adjusted NMI) | [-1, 1] | NMI adjusted for chance. Higher = more similar. | Conservative metric; penalizes random agreement; robust when clusters sizes vary. |

---

## Overlap-Based Metrics

| Metric | Range | Interpretation | Use Case |
|------|------|---------------|---------|
| PO (Partition Overlap) | [0, 1] | Proportion of term pairs in same cluster in both partitions. Higher = more overlap. | Intuitive; measures pairwise agreement on "sameness." |
| NPO (Normalized Partition Overlap) | [0, 1] | PO adjusted for partition sizes. Higher = more overlap. | Corrects for expected overlap due to random partitioning. |

---

## Rank-Based Metrics

| Metric | Range | Interpretation | Use Case |
|------|------|---------------|---------|
| ARI (Adjusted Rand Index) | [-1, 1] | Agreement on pairs of objects (same/different cluster). Higher = more similar. | Chance-adjusted; -1 = opposite structure, 0 = random, 1 = identical. |
| RMI (Reduced Mutual Information) | [0, ∞) | MI of co-occurrence patterns. Higher = more similar structure. | Focuses on block co-occurrence; sensitive to block organization. |
| NRMI (Normalized RMI) | [0, 1] | RMI normalized by entropy. Higher = more similar. | Scales RMI to [0,1]; comparable across different data sizes. |


All metrics are computed across all runs and averaged per window pair.

---

## Run Instructions

To execute a full experimental pipeline for a fixed sample:

1. From the root directory (`Word_Embedding/`), run:

```bash
python3 -m word_embedding.window_experiments --samples 5 --runs 5 --seed 12345 --windows 5 10 15 20 30 40 50 full
```

2. Compute metrics across all executions:

```bash
python3 word_embedding/compute_partition_metrics.py
```

3. Generate heatmaps:

```bash
python3 word_embedding/plot_average_heatmap.py
```

Each step is fully reproducible via the provided `--seed`.

---

## Results Summary

The experiments were conducted using controlled samples with fixed seeds and repeated over multiple runs in order to assess robustness, stability, and reproducibility.

Key conclusions, consistent with the findings of the previous study, are summarized below:

* **Structural robustness within methods**  
  High NMI values in intra-method comparisons (SBM × SBM and W2V × W2V) indicate that, for a given configuration, the main partitioning patterns are consistently recovered across runs. This shows that the inferred structures are not artifacts of stochasticity, but reflect robust properties induced by the graph construction or the embedding space.

* **Semantic–structural complementarity rather than equivalence**  
  Cross-model comparisons (SBM × W2V) exhibit partial agreement depending on window size and corpus scale. This confirms that structural and embedding-based approaches capture complementary aspects of semantic organization, rather than converging systematically to identical partitions.

* **Effect of corpus scale**  
  Increasing the number of samples leads to a global decrease in average NMI, reflecting the growing semantic heterogeneity of the corpus. This behavior, already observed in the previous work, highlights that larger corpora naturally introduce diversification of contexts and usages, making partition agreement more challenging.

* **Window size as a contextual scale parameter**  
  The sliding window plays a critical role in shaping the induced structure. Extreme window sizes (small windows and full context) tend to become more stable as the corpus grows, benefiting respectively from reinforced local co-occurrence patterns and clearer global thematic organization.  
  In contrast, intermediate window sizes show reduced agreement at larger scales, revealing a transition regime where semantic diversification is more strongly expressed.

Overall, these results confirm that context is not a single-scale phenomenon, and that the choice of window size determines which semantic regularities are emphasized or fragmented by structural modeling.


---

## Future Directions

* **Hybrid modeling**: use embeddings as node attributes in SBMs.
* **Interactive interfaces**: allow real-time visual tuning of window sizes.
* **Divergence-as-signal**: treat high disagreement as useful input for qualitative analysis.

---

Instead of forcing a single view, this project emphasizes the **productive tension** between structure and semantics — and how that tension can be **measured, visualized, and leveraged**.
`````
